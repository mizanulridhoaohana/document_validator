{"cells":[{"cell_type":"markdown","metadata":{"id":"yNveqeA1KXGy"},"source":["# Step 1: Install Requirements"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":46789,"status":"ok","timestamp":1693447025445,"user":{"displayName":"ML Training","userId":"00202169435591497317"},"user_tz":-480},"id":"kTvDNSILZoN9","outputId":"b633eb34-c499-4700-e36b-8ec58d98cec6"},"outputs":[{"name":"stdout","output_type":"stream","text":["fatal: destination path 'yolov5' already exists and is not an empty directory.\n"]},{"name":"stdout","output_type":"stream","text":["/home/mizanul/Documents/code/document_validator/yolov5\n","Note: you may need to restart the kernel to use updated packages.\n","Note: you may need to restart the kernel to use updated packages.\n","Setup complete. Using torch 2.0.1+cu117 (NVIDIA GeForce GTX 1650)\n"]}],"source":["#clone YOLOv5 and\n","# from google.colab import drive\n","# drive.mount('/content/drive', force_remount=True)\n","!git clone https://github.com/ultralytics/yolov5  # clone repo\n","%cd yolov5\n","%pip install -qr requirements.txt # install dependencies\n","%pip install -q roboflow\n","\n","import torch\n","import os\n","from IPython.display import Image, clear_output  # to display images\n","\n","print(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WsY9bQj-KAPq"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":113100,"status":"ok","timestamp":1693447499505,"user":{"displayName":"ML Training","userId":"00202169435591497317"},"user_tz":-480},"id":"FwJcaoPGF4VI","outputId":"f71b8f04-5b1e-46f5-f26c-1b953d69d3f6"},"outputs":[{"name":"stdout","output_type":"stream","text":["loading Roboflow workspace...\n","loading Roboflow project...\n","Downloading Dataset Version Zip in yolo_validator-2 to yolov5pytorch: 100% [18930111 / 18930111] bytes\n"]},{"name":"stderr","output_type":"stream","text":["Extracting Dataset Version Zip to yolo_validator-2 in yolov5pytorch:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1208/1208 [00:00<00:00, 5593.26it/s]\n"]}],"source":["# !pip install roboflow\n","\n","from roboflow import Roboflow\n","rf = Roboflow(api_key=\"fIx2dux10SnLtQWgAYdJ\")\n","project = rf.workspace(\"document-validator\").project(\"yolo_validator\")\n","version = project.version(2)\n","dataset = version.download(\"yolov5\")\n","                "]},{"cell_type":"markdown","metadata":{"id":"X7yAi9hd-T4B"},"source":["# Step 3: Train Our Custom YOLOv5 model\n","\n","Here, we are able to pass a number of arguments:\n","- **img:** define input image size\n","- **batch:** determine batch size\n","- **epochs:** define the number of training epochs. (Note: often, 3000+ are common here!)\n","- **data:** Our dataset locaiton is saved in the `dataset.location`\n","- **weights:** specify a path to weights to start transfer learning from. Here we choose the generic COCO pretrained checkpoint.\n","- **cache:** cache images for faster training"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# set up environment\n","# os.environ[\"DATASET_DIRECTORY\"] = \"/yolo_validator-2\""]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'/home/mizanul/Documents/code/document_validator/yolov5/yolo_validator-2'}\n"]}],"source":["print({dataset.location})"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["# %cd yolov5"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2593922,"status":"ok","timestamp":1693461621459,"user":{"displayName":"ML Training","userId":"00202169435591497317"},"user_tz":-480},"id":"eaFNnxLJbq4J","outputId":"c2da595d-6625-44a1-8fb8-9edb79efc5ce"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=, data=/home/mizanul/Documents/code/document_validator/yolov5/yolo_validator-2/data.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=2, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, evolve_population=data/hyps, resume_evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest, ndjson_console=False, ndjson_file=False\n","\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 âœ…\n","YOLOv5 ðŸš€ v7.0-361-gc5ffbbf1 Python-3.10.12 torch-2.0.1+cu117 CUDA:0 (NVIDIA GeForce GTX 1650, 3904MiB)\n","\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n","\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 ðŸš€ runs in Comet\n","\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n","Overriding model.yaml nc=80 with nc=3\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n","  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n","  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n","  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n","  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n","  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n","  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n","  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n","  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n","  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n"," 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n"," 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n"," 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n"," 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n"," 24      [17, 20, 23]  1     21576  models.yolo.Detect                      [3, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n","Model summary: 214 layers, 7027720 parameters, 7027720 gradients, 16.0 GFLOPs\n","\n","Transferred 343/349 items from yolov5s.pt\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n","\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/mizanul/Documents/code/document_validator/yolov5/yolo_vali\u001b[0m\n","\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.4GB ram): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 536/536 [00:00<00:00, 1303.7\u001b[0m\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /home/mizanul/Documents/code/document_validator/yolov5/yolo_valida\u001b[0m\n","\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.0GB ram): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:00<00:00, 299.17it/s\u001b[0m\n","\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0m5.84 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset âœ…\n","Plotting labels to runs/train/exp11/labels.jpg... \n","Image sizes 640 train, 640 val\n","Using 8 dataloader workers\n","Logging results to \u001b[1mruns/train/exp11\u001b[0m\n","Starting training for 2 epochs...\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        0/1      3.49G      0.105    0.02995    0.03498         21        640: 1\n","                 Class     Images  Instances          P          R      mAP50   \n","                   all         41         61          0          0          0          0\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        1/1      3.51G    0.07732     0.0272    0.02712         18        640: 1\n","                 Class     Images  Instances          P          R      mAP50   \n","                   all         41         61          0          0          0          0\n","\n","2 epochs completed in 0.030 hours.\n","Optimizer stripped from runs/train/exp11/weights/last.pt, 14.4MB\n","Optimizer stripped from runs/train/exp11/weights/best.pt, 14.4MB\n","\n","Validating runs/train/exp11/weights/best.pt...\n","Fusing layers... \n","Model summary: 157 layers, 7018216 parameters, 0 gradients, 15.8 GFLOPs\n","                 Class     Images  Instances          P          R      mAP50   \n","                   all         41         61          0          0          0          0\n","Results saved to \u001b[1mruns/train/exp11\u001b[0m\n"]}],"source":["!python3 train.py --img 640 --batch 16 --epochs 2 --data /home/mizanul/Documents/code/document_validator/yolov5/yolo_validator-2/data.yaml --weights yolov5s.pt --cache"]},{"cell_type":"markdown","metadata":{"id":"AcIRLQOlA14A"},"source":["# Evaluate Custom YOLOv5 Detector Performance\n","Training losses and performance metrics are saved to Tensorboard and also to a logfile.\n","\n","If you are new to these metrics, the one you want to focus on is `mAP_0.5` - learn more about mean average precision [here](https://blog.roboflow.com/mean-average-precision/)."]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":822},"executionInfo":{"elapsed":8554,"status":"ok","timestamp":1692584332875,"user":{"displayName":"ML Training","userId":"00202169435591497317"},"user_tz":-480},"id":"1jS9_BxdBBHL","outputId":"00c6848b-0a61-4924-bcf3-dfa50c23d831"},"outputs":[{"data":{"text/plain":["Launching TensorBoard..."]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","      <iframe id=\"tensorboard-frame-da13379729daa320\" width=\"100%\" height=\"800\" frameborder=\"0\">\n","      </iframe>\n","      <script>\n","        (function() {\n","          const frame = document.getElementById(\"tensorboard-frame-da13379729daa320\");\n","          const url = new URL(\"http://localhost\");\n","          const port = 6006;\n","          if (port) {\n","            url.port = port;\n","          }\n","          frame.src = url;\n","        })();\n","      </script>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["# Start tensorboard\n","# Launch after you have started training\n","# logs save in the folder \"runs\"\n","%load_ext tensorboard\n","%tensorboard --logdir runs"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[Errno 2] No such file or directory: '/home/mizanul/Documents/code-for-ta/yolov5'\n","/home/mizanul/Documents/code/document_validator/yolov5\n"]}],"source":["cd /home/mizanul/Documents/code-for-ta/yolov5"]},{"cell_type":"markdown","metadata":{"id":"jtmS7_TXFsT3"},"source":["#Run Inference  With Trained Weights\n","Run inference with a pretrained checkpoint on contents of `test/images` folder downloaded from Roboflow."]},{"cell_type":"code","execution_count":9,"metadata":{"id":"TWjjiBcic3Vz"},"outputs":[{"name":"stdout","output_type":"stream","text":["/bin/bash: line 1: python: command not found\n"]}],"source":["!python detect.py --weights runs/train/exp/weights/best.pt --img 416 --conf 0.1 --source Documents/code-for-ta/Testing_Video/test_1.mp4"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1GOILP19e8KlGwFaecBBMOT22RAXqmtVF"},"id":"ZbUn4_b9GCKO","outputId":"8dfafadc-f156-4f28-94b9-5d9171cef704"},"outputs":[],"source":["#display inference on ALL test images\n","\n","import glob\n","from IPython.display import Image, display\n","\n","for imageName in glob.glob('/content/yolov5/runs/detect/exp/*.jpg'): #assuming JPG\n","    display(Image(filename=imageName))\n","    print(\"\\n\")"]},{"cell_type":"markdown","metadata":{"id":"g8dHcni6CJYt"},"source":["# Conclusion and Next Steps\n","\n","Congratulations! You've trained a custom YOLOv5 model to recognize your custom objects.\n","\n","To improve you model's performance, we recommend first interating on your datasets coverage and quality. See this guide for [model performance improvement](https://github.com/ultralytics/yolov5/wiki/Tips-for-Best-Training-Results).\n","\n","To deploy your model to an application, see this guide on [exporting your model to deployment destinations](https://github.com/ultralytics/yolov5/issues/251).\n","\n","Once your model is in production, you will want to continually iterate and improve on your dataset and model via [active learning](https://blog.roboflow.com/what-is-active-learning/)."]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":17},"id":"7iiObB2WCMh6","outputId":"a392d19e-57e5-45ba-abb9-05f1243af98b"},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'google.colab'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#export your model's weights for future use\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m files\n\u001b[1;32m      3\u001b[0m files\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./runs/train/exp/weights/best.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"]}],"source":["#export your model's weights for future use\n","from google.colab import files\n","files.download('./runs/train/exp/weights/best.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rNn-obvOGITm"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"V100","provenance":[{"file_id":"https://github.com/roboflow-ai/yolov5-custom-training-tutorial/blob/main/yolov5-custom-training.ipynb","timestamp":1692152426875}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}
